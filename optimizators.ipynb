{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, initial_solution, calculate_gradient, learning_rate=0.01, max_num_epoch=1000):\n",
    "    \"\"\"\n",
    "    Performs stochastic gradient descent optimization.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data.\n",
    "    - y: Target labels.\n",
    "    - initial_solution: Initial solution for optimization.\n",
    "    - calculate_gradient: Function to calculate the gradient.\n",
    "    - learning_rate: Learning rate for updating the solution (default: 0.01).\n",
    "    - max_num_iters: Maximum number of iterations (default: 1000).\n",
    "\n",
    "    Returns:\n",
    "    - The optimized solution.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization\n",
    "    if type(X) is pd.DataFrame:\n",
    "        X = X.to_numpy()\n",
    "    if type(y) is pd.DataFrame:\n",
    "        y = y.to_numpy().T\n",
    "    current_solution = initial_solution \n",
    "\n",
    "    for _ in range(max_num_epoch):\n",
    "        N, _ = X.shape\n",
    "        shuffled_idx = np.random.permutation(N)\n",
    "        X, y = X[shuffled_idx], y[shuffled_idx]\n",
    "        for X_selected, y_selected in zip(X, y):\n",
    "            gradient = calculate_gradient(X_selected, y_selected, current_solution)\n",
    "            current_solution = current_solution - learning_rate * gradient\n",
    "        print('Epoch:', current_solution)\n",
    "    return current_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO\n",
    "\n",
    "1. implement stop condition - additional parameter loss_tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(\n",
    "    X, y, initial_solution, calculate_gradient, learning_rate=0.01, max_num_epoch=1000, batch_size=1, batch_fraction=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs mini batch gradient descent optimization.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data.\n",
    "    - y: Target labels.\n",
    "    - initial_solution: Initial solution for optimization.\n",
    "    - calculate_gradient: Function to calculate the gradient.\n",
    "    - learning_rate: Learning rate for updating the solution (default: 0.01).\n",
    "    - max_num_iters: Maximum number of iterations (default: 1000).\n",
    "    - batch_size: Size of the mini batch (default: 1).\n",
    "    - batch_fraction: Fraction of the data to use in each mini batch (default: None).\n",
    "\n",
    "    Returns:\n",
    "    - The optimized solution.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization\n",
    "    if type(X) is pd.DataFrame:\n",
    "        X = X.to_numpy()\n",
    "    if type(y) is pd.DataFrame:\n",
    "        y = y.to_numpy().T\n",
    "    current_solution = initial_solution\n",
    "\n",
    "    # set batch size\n",
    "    assert type(batch_size) is int, \"batch_size must be an integer\"\n",
    "    if batch_fraction is not None:\n",
    "        assert 0 < batch_fraction <= 1, \"batch_fraction must be between 0 and 1\"\n",
    "        batch_size = int(X.shape[0] * batch_fraction)\n",
    "    iterations = int(X.shape[0] / batch_size)\n",
    "\n",
    "    for _ in range(max_num_epoch):\n",
    "        N, _ = X.shape\n",
    "        shuffled_idx = np.random.permutation(N)\n",
    "        X, y = X[shuffled_idx], y[shuffled_idx]\n",
    "        for idx in range(iterations):\n",
    "            X_selected, y_selected = X[idx * batch_size : (idx + 1) * batch_size], y[idx * batch_size : (idx + 1) * batch_size]\n",
    "            gradient = calculate_gradient(X_selected, y_selected, current_solution)\n",
    "            current_solution = current_solution - learning_rate * gradient\n",
    "        print(\"Epoch:\", current_solution)\n",
    "    return current_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini batch gradient descent with momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent_with_momentum(\n",
    "    X,\n",
    "    y,\n",
    "    initial_solution,\n",
    "    calculate_gradient,\n",
    "    learning_rate=0.01,\n",
    "    momentum_decay=0.9,\n",
    "    max_num_epoch=1000,\n",
    "    batch_size=1,\n",
    "    batch_fraction=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs mini batch gradient descent with momentum optimization.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data.\n",
    "    - y: Target labels.\n",
    "    - initial_solution: Initial solution for optimization.\n",
    "    - calculate_gradient: Function to calculate the gradient.\n",
    "    - learning_rate: Learning rate for updating the solution (default: 0.01).\n",
    "    - momentum_decay: Decay rate for the momentum (default: 0.9).\n",
    "    - max_num_iters: Maximum number of iterations (default: 1000).\n",
    "    - batch_size: Size of the mini batch (default: 1).\n",
    "    - batch_fraction: Fraction of the data to use in each mini batch (default: None).\n",
    "\n",
    "    Returns:\n",
    "    - The optimized solution.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization\n",
    "    if type(X) is pd.DataFrame:\n",
    "        X = X.to_numpy()\n",
    "    if type(y) is pd.DataFrame:\n",
    "        y = y.to_numpy().T\n",
    "    current_solution = initial_solution\n",
    "    momentum = np.zeros_like(initial_solution)\n",
    "    \n",
    "\n",
    "    # set batch size\n",
    "    assert type(batch_size) is int, \"batch_size must be an integer\"\n",
    "    if batch_fraction is not None:\n",
    "        assert 0 < batch_fraction <= 1, \"batch_fraction must be between 0 and 1\"\n",
    "        batch_size = int(X.shape[0] * batch_fraction)\n",
    "    iterations = int(X.shape[0] / batch_size)\n",
    "\n",
    "    for _ in range(max_num_epoch):\n",
    "        N, _ = X.shape\n",
    "        shuffled_idx = np.random.permutation(N)\n",
    "        X, y = X[shuffled_idx], y[shuffled_idx]\n",
    "        for idx in range(iterations):\n",
    "            X_selected, y_selected = (\n",
    "                X[idx * batch_size : (idx + 1) * batch_size],\n",
    "                y[idx * batch_size : (idx + 1) * batch_size],\n",
    "            )\n",
    "            gradient = calculate_gradient(X_selected, y_selected, current_solution)\n",
    "            momentum = momentum_decay * momentum - learning_rate * gradient\n",
    "            current_solution = current_solution + momentum\n",
    "        print(\"Epoch:\", current_solution)\n",
    "    return current_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad(\n",
    "    X,\n",
    "    y,\n",
    "    initial_solution,\n",
    "    calculate_gradient,\n",
    "    learning_rate=0.01,\n",
    "    max_num_epoch=1000,\n",
    "    batch_size=1,\n",
    "    batch_fraction=None,\n",
    "    epsilon=1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs adagrad optimization.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data.\n",
    "    - y: Target labels.\n",
    "    - initial_solution: Initial solution for optimization.\n",
    "    - calculate_gradient: Function to calculate the gradient.\n",
    "    - learning_rate: Learning rate for updating the solution (default: 0.01).\n",
    "    - max_num_iters: Maximum number of iterations (default: 1000).\n",
    "    - batch_size: Size of the mini batch (default: 1).\n",
    "    - batch_fraction: Fraction of the data to use in each mini batch (default: None).\n",
    "    - epsilon: Small value to avoid division by zero (default: 1e-8).\n",
    "\n",
    "    Returns:\n",
    "    - The optimized solution.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization\n",
    "    if type(X) is pd.DataFrame:\n",
    "        X = X.to_numpy()\n",
    "    if type(y) is pd.DataFrame:\n",
    "        y = y.to_numpy().T\n",
    "    current_solution = initial_solution\n",
    "    squared_gradients = np.zeros_like(initial_solution)\n",
    "\n",
    "    # set batch size\n",
    "    assert type(batch_size) is int, \"batch_size must be an integer\"\n",
    "    if batch_fraction is not None:\n",
    "        assert 0 < batch_fraction <= 1, \"batch_fraction must be between 0 and 1\"\n",
    "        batch_size = int(X.shape[0] * batch_fraction)\n",
    "    iterations = int(X.shape[0] / batch_size)\n",
    "\n",
    "    for _ in range(max_num_epoch):\n",
    "        N, _ = X.shape\n",
    "        shuffled_idx = np.random.permutation(N)\n",
    "        X, y = X[shuffled_idx], y[shuffled_idx]\n",
    "        for idx in range(iterations):\n",
    "            X_selected, y_selected = (\n",
    "                X[idx * batch_size : (idx + 1) * batch_size],\n",
    "                y[idx * batch_size : (idx + 1) * batch_size],\n",
    "            )\n",
    "            gradient = calculate_gradient(X_selected, y_selected, current_solution)\n",
    "            squared_gradients += gradient ** 2\n",
    "            current_solution = current_solution - learning_rate * gradient / (np.sqrt(squared_gradients) + epsilon)\n",
    "        print(\"Epoch:\", current_solution)\n",
    "    return current_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(\n",
    "    X,\n",
    "    y,\n",
    "    initial_solution,\n",
    "    calculate_gradient,\n",
    "    learning_rate=0.01,\n",
    "    squared_gradient_decay=0.99,\n",
    "    max_num_epoch=1000,\n",
    "    batch_size=1,\n",
    "    batch_fraction=None,\n",
    "    epsilon=1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "       Performs RMSProp optimization.\n",
    "\n",
    "       Parameters:\n",
    "       - X: Input data.\n",
    "       - y: Target labels.\n",
    "       - initial_solution: Initial solution for optimization.\n",
    "       - calculate_gradient: Function to calculate the gradient.\n",
    "       - learning_rate: Learning rate for updating the solution (default: 0.01).\n",
    "       - squared_gradient_decay: Decay rate for the squared gradient (default: 0.99).\n",
    "       - max_num_iters: Maximum number of iterations (default: 1000).\n",
    "       - batch_size: Size of the mini batch (default: 1).\n",
    "       - batch_fraction: Fraction of the data to use in each mini batch (default: None).\n",
    "       - epsilon: Small value to avoid division by zero (default: 1e-8).\n",
    "\n",
    "       Returns:\n",
    "       - The optimized solution.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization\n",
    "    if type(X) is pd.DataFrame:\n",
    "        X = X.to_numpy()\n",
    "    if type(y) is pd.DataFrame:\n",
    "        y = y.to_numpy().T\n",
    "    current_solution = initial_solution\n",
    "    squared_gradients = np.zeros_like(initial_solution)\n",
    "\n",
    "    # set batch size\n",
    "    assert type(batch_size) is int, \"batch_size must be an integer\"\n",
    "    if batch_fraction is not None:\n",
    "        assert 0 < batch_fraction <= 1, \"batch_fraction must be between 0 and 1\"\n",
    "        batch_size = int(X.shape[0] * batch_fraction)\n",
    "    iterations = int(X.shape[0] / batch_size)\n",
    "\n",
    "    for _ in range(max_num_epoch):\n",
    "        N, _ = X.shape\n",
    "        shuffled_idx = np.random.permutation(N)\n",
    "        X, y = X[shuffled_idx], y[shuffled_idx]\n",
    "        for idx in range(iterations):\n",
    "            X_selected, y_selected = (\n",
    "                X[idx * batch_size : (idx + 1) * batch_size],\n",
    "                y[idx * batch_size : (idx + 1) * batch_size],\n",
    "            )\n",
    "            gradient = calculate_gradient(X_selected, y_selected, current_solution)\n",
    "            squared_gradients = squared_gradient_decay * squared_gradients + (1 - squared_gradient_decay) * gradient**2\n",
    "            current_solution = current_solution - learning_rate * gradient / (\n",
    "                np.sqrt(squared_gradients) + epsilon\n",
    "            )\n",
    "        print(\"Epoch:\", current_solution)\n",
    "    return current_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(\n",
    "    X,\n",
    "    y,\n",
    "    initial_solution,\n",
    "    calculate_gradient,\n",
    "    learning_rate=0.01,\n",
    "    momentum_decay=0.9,\n",
    "    squared_gradient_decay=0.99,\n",
    "    max_num_epoch=1000,\n",
    "    batch_size=1,\n",
    "    batch_fraction=None,\n",
    "    epsilon=1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs optimization with adam algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data.\n",
    "    - y: Target labels.\n",
    "    - initial_solution: Initial solution for optimization.\n",
    "    - calculate_gradient: Function to calculate the gradient.\n",
    "    - learning_rate: Learning rate for updating the solution (default: 0.01).\n",
    "    - momentum_decay: Decay rate for the momentum (default: 0.9).\n",
    "    - squared_gradient_decay: Decay rate for the squared gradient (default: 0.99).\n",
    "    - max_num_iters: Maximum number of iterations (default: 1000).\n",
    "    - batch_size: Size of the mini batch (default: 1).\n",
    "    - batch_fraction: Fraction of the data to use in each mini batch (default: None).\n",
    "    - epsilon: Small value to avoid division by zero (default: 1e-8).\n",
    "\n",
    "    Returns:\n",
    "    - The optimized solution.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialization\n",
    "    if type(X) is pd.DataFrame:\n",
    "        X = X.to_numpy()\n",
    "    if type(y) is pd.DataFrame:\n",
    "        y = y.to_numpy().T\n",
    "    current_solution = initial_solution\n",
    "    momentum = np.zeros_like(initial_solution)\n",
    "    squared_gradients = np.zeros_like(initial_solution)\n",
    "    counter = 0\n",
    "\n",
    "    # set batch size\n",
    "    assert type(batch_size) is int, \"batch_size must be an integer\"\n",
    "    if batch_fraction is not None:\n",
    "        assert 0 < batch_fraction <= 1, \"batch_fraction must be between 0 and 1\"\n",
    "        batch_size = int(X.shape[0] * batch_fraction)\n",
    "    iterations = int(X.shape[0] / batch_size)\n",
    "\n",
    "    for _ in range(max_num_epoch):\n",
    "        N, _ = X.shape\n",
    "        shuffled_idx = np.random.permutation(N)\n",
    "        X, y = X[shuffled_idx], y[shuffled_idx]\n",
    "        for idx in range(iterations):\n",
    "            X_selected, y_selected = (\n",
    "                X[idx * batch_size : (idx + 1) * batch_size],\n",
    "                y[idx * batch_size : (idx + 1) * batch_size],\n",
    "            )\n",
    "            gradient = calculate_gradient(X_selected, y_selected, current_solution)\n",
    "            momentum = momentum_decay * momentum + (1 - momentum_decay) * gradient\n",
    "            squared_gradients = (\n",
    "                squared_gradient_decay * squared_gradients\n",
    "                + (1 - squared_gradient_decay) * gradient**2\n",
    "            )\n",
    "            counter += 1\n",
    "\n",
    "            # bias correction\n",
    "            corrected_momentum = momentum / (1 - momentum_decay**counter)\n",
    "            corrected_squared_gradients = squared_gradients / (1 - squared_gradient_decay**counter)\n",
    "\n",
    "            current_solution = current_solution - learning_rate * corrected_momentum / (\n",
    "                np.sqrt(corrected_squared_gradients) + epsilon\n",
    "            )\n",
    "\n",
    "        print(\"Epoch:\", current_solution)\n",
    "    return current_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asseco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
